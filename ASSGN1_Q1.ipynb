{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ASSGN1_Q1",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ydx0PKklRR3o",
        "outputId": "917251b0-9631-4c4a-d2e7-0022149a5018"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "wLCVgAkw6Jze"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cost_function(X,y,w): ###define cost function\n",
        "  hypothesis = np.dot(X,w.T) ###calculation of hypothesis for all instances\n",
        "  J = (1/(2*len(y))) * np.sum((hypothesis - y) ** 2) ####as mention in the class notes\n",
        "  # J = (1/(2*len(y))) * np.sum((hypothesis - y) ** 2)+(lamb/2)*np.sum(w**2) ####as mention\n",
        "  return J"
      ],
      "metadata": {
        "id": "TRzyVoes6LSV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_gradient_descent(X,y,w,alpha,iters,lamb):\n",
        "  cost_history = np.zeros(iters) # cost function for each iteration\n",
        "  #initalize our cost history list to store the cost function on every iteration\n",
        "  for i in range(iters):\n",
        "    hypothesis = np.dot(X,w.T)\n",
        "    #w = (w*(1-alpha*lamb)) -(alpha/len(y)) * np.dot(hypothesis - y, X)\n",
        "    w = w - (alpha/len(y)) * np.dot(hypothesis - y, X)\n",
        "    #cost_history[i] = cost_function(X,y,w,lamb)\n",
        "    cost_history[i] = cost_function(X,y,w)\n",
        "  return w,cost_history"
      ],
      "metadata": {
        "id": "TRsfyuOK6QJf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MB_gradient_descent(X,y,w,alpha, iters, batch_size):\n",
        "  cost_history = np.zeros(iters)\n",
        "  for i in range(iters):\n",
        "    rand_index = np.random.randint(len(y)-batch_size)\n",
        "    ind_x = X[rand_index:rand_index+batch_size]\n",
        "    ind_y = y[rand_index:rand_index+batch_size]\n",
        "    w = w - (alpha/batch_size) * (ind_x.T.dot(ind_x.dot(w) - ind_y))\n",
        "    cost_history[i] = cost_function(ind_x,ind_y,w)\n",
        "  return w, cost_history"
      ],
      "metadata": {
        "id": "l7neEv2h6R6Z"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stochastic_gradient_descent(X,y,w,alpha, iters):\n",
        "  cost_history = np.zeros(iters)\n",
        "  for i in range(iters):\n",
        "    rand_index = np.random.randint(len(y)-1)\n",
        "    ind_x = X[rand_index:rand_index+1]\n",
        "    ind_y = y[rand_index:rand_index+1]\n",
        "    w = w - alpha * (ind_x.T.dot(ind_x.dot(w) - ind_y))\n",
        "    cost_history[i] = cost_function(ind_x,ind_y,w)\n",
        "  return w, cost_history"
      ],
      "metadata": {
        "id": "BsZ2V59s6TmP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "trainingData"
      ],
      "metadata": {
        "id": "770tYAsTcRCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataxtr = pd.read_csv('/content/drive/MyDrive/xtr.csv',header=None)\n",
        "dataytr = pd.read_csv('/content/drive/MyDrive/ytr.csv',header=None)\n"
      ],
      "metadata": {
        "id": "pN6ykAyeikel"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TestingData"
      ],
      "metadata": {
        "id": "wp6X92ClcVDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataxte = pd.read_csv('/content/drive/MyDrive/xte.csv',header=None)\n",
        "datayte = pd.read_csv('/content/drive/MyDrive/yte.csv',header=None)\n"
      ],
      "metadata": {
        "id": "kFtlAWlccUum"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_XTraining=dataxtr.values\n",
        "X=data_XTraining[:,:]\n",
        "# print(X)\n",
        "m=X.shape[0]\n",
        "xmin=np.min(X,axis=0)\n",
        "xmax=np.max(X,axis=0)\n",
        "# print(xmin , xmax)\n",
        "X = (X- xmin)/(xmax-xmin) #Normalization\n",
        "# print(X)"
      ],
      "metadata": {
        "id": "Edy94uT_YAzC"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pp = np.ones([m, 1]) # vector containg ones as all elements\n",
        "X = np.append(pp,X, axis=1) #Column of ones\n",
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnl-xwugYA7S",
        "outputId": "308f47a2-787f-4314-ab41-fbe4124a534a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.         0.51322751 0.0729927 ]\n",
            " [1.         0.51322751 0.74452555]\n",
            " [1.         0.51322751 0.62043796]\n",
            " [1.         0.51322751 0.        ]\n",
            " [1.         0.51322751 0.6350365 ]\n",
            " [1.         0.51322751 0.02919708]\n",
            " [1.         0.51322751 0.57664234]\n",
            " [1.         0.51322751 0.64233577]\n",
            " [1.         0.51322751 0.11678832]\n",
            " [1.         0.51322751 0.64963504]\n",
            " [1.         0.51322751 0.62043796]\n",
            " [1.         0.74603175 0.80291971]\n",
            " [1.         0.97883598 0.43065693]\n",
            " [1.         0.25396825 0.24817518]\n",
            " [1.         0.02116402 0.62773723]\n",
            " [1.         0.51322751 0.81021898]\n",
            " [1.         0.74603175 1.        ]\n",
            " [1.         0.97354497 0.44525547]\n",
            " [1.         0.02116402 0.25547445]\n",
            " [1.         0.50793651 0.64233577]\n",
            " [1.         0.73544974 0.82481752]\n",
            " [1.         0.97354497 1.        ]\n",
            " [1.         0.26984127 0.45985401]\n",
            " [1.         0.02116402 0.28467153]\n",
            " [1.         0.51322751 0.6350365 ]\n",
            " [1.         0.73015873 0.81021898]\n",
            " [1.         0.96296296 1.        ]\n",
            " [1.         0.25925926 0.45255474]\n",
            " [1.         0.02116402 0.27007299]\n",
            " [1.         0.51322751 0.61313869]\n",
            " [1.         0.75661376 0.80291971]\n",
            " [1.         0.98941799 0.98175182]\n",
            " [1.         0.25925926 0.43065693]\n",
            " [1.         0.         0.25182482]\n",
            " [1.         0.51322751 0.61313869]\n",
            " [1.         0.75132275 0.79927007]\n",
            " [1.         0.97883598 0.97810219]\n",
            " [1.         0.26984127 0.4379562 ]\n",
            " [1.         0.01587302 0.25547445]\n",
            " [1.         0.51322751 0.86131387]\n",
            " [1.         0.51322751 0.74452555]\n",
            " [1.         0.51322751 0.60583942]\n",
            " [1.         0.51322751 0.61313869]\n",
            " [1.         0.51322751 0.60583942]\n",
            " [1.         0.51322751 0.61313869]\n",
            " [1.         0.51322751 0.59124088]\n",
            " [1.         0.76719577 0.77372263]\n",
            " [1.         1.         0.95620438]\n",
            " [1.         0.28042328 0.40875912]\n",
            " [1.         0.02645503 0.22627737]\n",
            " [1.         0.51322751 0.59124088]\n",
            " [1.         0.75132275 0.77372263]\n",
            " [1.         0.98412698 0.95620438]\n",
            " [1.         0.26455026 0.40875912]\n",
            " [1.         0.01587302 0.22627737]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mnz2SVGshJPK",
        "outputId": "5f237247-9c30-42b7-a85e-d390d55f09bc"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(55, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dGNyLW1bhN3h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}